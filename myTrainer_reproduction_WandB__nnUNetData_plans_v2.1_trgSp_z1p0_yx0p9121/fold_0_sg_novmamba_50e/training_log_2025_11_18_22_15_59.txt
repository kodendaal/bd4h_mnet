Starting... 
2025-11-18 22:15:59.677608: Training config: max_epochs=1, gated_fusion=spatial 
2025-11-18 22:16:00.366666: Model params: total=8,769,484, trainable=8,769,484 
2025-11-18 22:16:05.374784: Unable to plot network architecture: 
2025-11-18 22:16:05.384904: No module named 'hiddenlayer' 
2025-11-18 22:16:05.396529: 
printing the network instead:
 
2025-11-18 22:16:05.408812: MNet(
  (down11): Down(
    (CB2d): CB3d(
      (conv1): CNA3d(
        (conv): Conv3d(1, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
        (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (conv2): CNA3d(
        (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
        (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
    )
    (CB3d): CB3d(
      (conv1): CNA3d(
        (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (conv2): CNA3d(
        (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
    )
  )
  (down12): Down(
    (CB2d): CB3d(
      (conv1): CNA3d(
        (conv): Conv3d(32, 48, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
        (norm): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (conv2): CNA3d(
        (conv): Conv3d(48, 48, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
        (norm): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
    )
    (CB3d): CB3d(
      (conv1): CNA3d(
        (conv): Conv3d(32, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        (norm): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (conv2): CNA3d(
        (conv): Conv3d(48, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        (norm): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
    )
  )
  (down13): Down(
    (CB2d): CB3d(
      (conv1): CNA3d(
        (conv): Conv3d(48, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
        (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (conv2): CNA3d(
        (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
        (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
    )
    (CB3d): CB3d(
      (conv1): CNA3d(
        (conv): Conv3d(48, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (conv2): CNA3d(
        (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
    )
  )
  (down14): Down(
    (CB2d): CB3d(
      (conv1): CNA3d(
        (conv): Conv3d(64, 80, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
        (norm): InstanceNorm3d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (conv2): CNA3d(
        (conv): Conv3d(80, 80, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
        (norm): InstanceNorm3d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
    )
    (CB3d): CB3d(
      (conv1): CNA3d(
        (conv): Conv3d(64, 80, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        (norm): InstanceNorm3d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (conv2): CNA3d(
        (conv): Conv3d(80, 80, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        (norm): InstanceNorm3d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
    )
  )
  (bottleneck1): Down(
    (CB2d): CB3d(
      (conv1): CNA3d(
        (conv): Conv3d(80, 96, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
        (norm): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (conv2): CNA3d(
        (conv): Conv3d(96, 96, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
        (norm): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
    )
  )
  (up11): Up(
    (CB2d): CB3d(
      (conv1): CNA3d(
        (conv): Conv3d(176, 80, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
        (norm): InstanceNorm3d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (conv2): CNA3d(
        (conv): Conv3d(80, 80, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
        (norm): InstanceNorm3d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
    )
    (fuse): GatedFMU(
      (sg): SpatialGate(
        (conv): Conv3d(2, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      )
    )
    (fuse_skip): GatedFMU(
      (sg): SpatialGate(
        (conv): Conv3d(2, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      )
    )
    (fuse_up): GatedFMU(
      (sg): SpatialGate(
        (conv): Conv3d(2, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      )
    )
  )
  (up12): Up(
    (CB2d): CB3d(
      (conv1): CNA3d(
        (conv): Conv3d(144, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
        (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (conv2): CNA3d(
        (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
        (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
    )
    (fuse): GatedFMU(
      (sg): SpatialGate(
        (conv): Conv3d(2, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      )
    )
    (fuse_skip): GatedFMU(
      (sg): SpatialGate(
        (conv): Conv3d(2, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      )
    )
    (fuse_up): GatedFMU(
      (sg): SpatialGate(
        (conv): Conv3d(2, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      )
    )
  )
  (up13): Up(
    (CB2d): CB3d(
      (conv1): CNA3d(
        (conv): Conv3d(112, 48, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
        (norm): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (conv2): CNA3d(
        (conv): Conv3d(48, 48, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
        (norm): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
    )
    (fuse): GatedFMU(
      (sg): SpatialGate(
        (conv): Conv3d(2, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      )
    )
    (fuse_skip): GatedFMU(
      (sg): SpatialGate(
        (conv): Conv3d(2, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      )
    )
    (fuse_up): GatedFMU(
      (sg): SpatialGate(
        (conv): Conv3d(2, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      )
    )
  )
  (up14): Up(
    (CB2d): CB3d(
      (conv1): CNA3d(
        (conv): Conv3d(80, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
        (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (conv2): CNA3d(
        (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
        (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
    )
    (CB3d): CB3d(
      (conv1): CNA3d(
        (conv): Conv3d(80, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (conv2): CNA3d(
        (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
    )
    (fuse): GatedFMU(
      (sg): SpatialGate(
        (conv): Conv3d(2, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      )
    )
    (fuse_skip): GatedFMU(
      (sg): SpatialGate(
        (conv): Conv3d(2, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      )
    )
    (fuse_up): GatedFMU(
      (sg): SpatialGate(
        (conv): Conv3d(2, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      )
    )
  )
  (down21): Down(
    (CB2d): CB3d(
      (conv1): CNA3d(
        (conv): Conv3d(32, 48, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
        (norm): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (conv2): CNA3d(
        (conv): Conv3d(48, 48, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
        (norm): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
    )
    (CB3d): CB3d(
      (conv1): CNA3d(
        (conv): Conv3d(32, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        (norm): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (conv2): CNA3d(
        (conv): Conv3d(48, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        (norm): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
    )
  )
  (down22): Down(
    (CB2d): CB3d(
      (conv1): CNA3d(
        (conv): Conv3d(48, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
        (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (conv2): CNA3d(
        (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
        (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
    )
    (CB3d): CB3d(
      (conv1): CNA3d(
        (conv): Conv3d(48, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (conv2): CNA3d(
        (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
    )
    (fuse): GatedFMU(
      (sg): SpatialGate(
        (conv): Conv3d(2, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      )
    )
  )
  (down23): Down(
    (CB2d): CB3d(
      (conv1): CNA3d(
        (conv): Conv3d(64, 80, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
        (norm): InstanceNorm3d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (conv2): CNA3d(
        (conv): Conv3d(80, 80, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
        (norm): InstanceNorm3d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
    )
    (CB3d): CB3d(
      (conv1): CNA3d(
        (conv): Conv3d(64, 80, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        (norm): InstanceNorm3d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (conv2): CNA3d(
        (conv): Conv3d(80, 80, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        (norm): InstanceNorm3d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
    )
    (fuse): GatedFMU(
      (sg): SpatialGate(
        (conv): Conv3d(2, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      )
    )
  )
  (bottleneck2): Down(
    (CB2d): CB3d(
      (conv1): CNA3d(
        (conv): Conv3d(80, 96, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
        (norm): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (conv2): CNA3d(
        (conv): Conv3d(96, 96, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
        (norm): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
    )
    (CB3d): CB3d(
      (conv1): CNA3d(
        (conv): Conv3d(80, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        (norm): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (conv2): CNA3d(
        (conv): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        (norm): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
    )
    (fuse): GatedFMU(
      (sg): SpatialGate(
        (conv): Conv3d(2, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      )
    )
  )
  (up21): Up(
    (CB2d): CB3d(
      (conv1): CNA3d(
        (conv): Conv3d(176, 80, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
        (norm): InstanceNorm3d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (conv2): CNA3d(
        (conv): Conv3d(80, 80, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
        (norm): InstanceNorm3d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
    )
    (CB3d): CB3d(
      (conv1): CNA3d(
        (conv): Conv3d(176, 80, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        (norm): InstanceNorm3d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (conv2): CNA3d(
        (conv): Conv3d(80, 80, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        (norm): InstanceNorm3d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
    )
    (fuse): GatedFMU(
      (sg): SpatialGate(
        (conv): Conv3d(2, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      )
    )
    (fuse_skip): GatedFMU(
      (sg): SpatialGate(
        (conv): Conv3d(2, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      )
    )
    (fuse_up): GatedFMU(
      (sg): SpatialGate(
        (conv): Conv3d(2, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      )
    )
  )
  (up22): Up(
    (CB2d): CB3d(
      (conv1): CNA3d(
        (conv): Conv3d(144, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
        (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (conv2): CNA3d(
        (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
        (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
    )
    (CB3d): CB3d(
      (conv1): CNA3d(
        (conv): Conv3d(144, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (conv2): CNA3d(
        (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
    )
    (fuse): GatedFMU(
      (sg): SpatialGate(
        (conv): Conv3d(2, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      )
    )
    (fuse_skip): GatedFMU(
      (sg): SpatialGate(
        (conv): Conv3d(2, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      )
    )
    (fuse_up): GatedFMU(
      (sg): SpatialGate(
        (conv): Conv3d(2, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      )
    )
  )
  (up23): Up(
    (CB3d): CB3d(
      (conv1): CNA3d(
        (conv): Conv3d(112, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        (norm): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (conv2): CNA3d(
        (conv): Conv3d(48, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        (norm): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
    )
    (fuse): GatedFMU(
      (sg): SpatialGate(
        (conv): Conv3d(2, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      )
    )
    (fuse_skip): GatedFMU(
      (sg): SpatialGate(
        (conv): Conv3d(2, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      )
    )
    (fuse_up): GatedFMU(
      (sg): SpatialGate(
        (conv): Conv3d(2, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      )
    )
  )
  (down31): Down(
    (CB2d): CB3d(
      (conv1): CNA3d(
        (conv): Conv3d(48, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
        (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (conv2): CNA3d(
        (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
        (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
    )
    (CB3d): CB3d(
      (conv1): CNA3d(
        (conv): Conv3d(48, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (conv2): CNA3d(
        (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
    )
  )
  (down32): Down(
    (CB2d): CB3d(
      (conv1): CNA3d(
        (conv): Conv3d(64, 80, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
        (norm): InstanceNorm3d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (conv2): CNA3d(
        (conv): Conv3d(80, 80, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
        (norm): InstanceNorm3d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
    )
    (CB3d): CB3d(
      (conv1): CNA3d(
        (conv): Conv3d(64, 80, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        (norm): InstanceNorm3d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (conv2): CNA3d(
        (conv): Conv3d(80, 80, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        (norm): InstanceNorm3d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
    )
    (fuse): GatedFMU(
      (sg): SpatialGate(
        (conv): Conv3d(2, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      )
    )
  )
  (bottleneck3): Down(
    (CB2d): CB3d(
      (conv1): CNA3d(
        (conv): Conv3d(80, 96, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
        (norm): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (conv2): CNA3d(
        (conv): Conv3d(96, 96, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
        (norm): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
    )
    (CB3d): CB3d(
      (conv1): CNA3d(
        (conv): Conv3d(80, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        (norm): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (conv2): CNA3d(
        (conv): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        (norm): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
    )
    (fuse): GatedFMU(
      (sg): SpatialGate(
        (conv): Conv3d(2, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      )
    )
  )
  (up31): Up(
    (CB2d): CB3d(
      (conv1): CNA3d(
        (conv): Conv3d(176, 80, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
        (norm): InstanceNorm3d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (conv2): CNA3d(
        (conv): Conv3d(80, 80, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
        (norm): InstanceNorm3d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
    )
    (CB3d): CB3d(
      (conv1): CNA3d(
        (conv): Conv3d(176, 80, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        (norm): InstanceNorm3d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (conv2): CNA3d(
        (conv): Conv3d(80, 80, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        (norm): InstanceNorm3d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
    )
    (fuse): GatedFMU(
      (sg): SpatialGate(
        (conv): Conv3d(2, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      )
    )
    (fuse_skip): GatedFMU(
      (sg): SpatialGate(
        (conv): Conv3d(2, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      )
    )
    (fuse_up): GatedFMU(
      (sg): SpatialGate(
        (conv): Conv3d(2, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      )
    )
  )
  (up32): Up(
    (CB3d): CB3d(
      (conv1): CNA3d(
        (conv): Conv3d(144, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (conv2): CNA3d(
        (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
    )
    (fuse): GatedFMU(
      (sg): SpatialGate(
        (conv): Conv3d(2, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      )
    )
    (fuse_skip): GatedFMU(
      (sg): SpatialGate(
        (conv): Conv3d(2, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      )
    )
    (fuse_up): GatedFMU(
      (sg): SpatialGate(
        (conv): Conv3d(2, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      )
    )
  )
  (down41): Down(
    (CB2d): CB3d(
      (conv1): CNA3d(
        (conv): Conv3d(64, 80, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
        (norm): InstanceNorm3d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (conv2): CNA3d(
        (conv): Conv3d(80, 80, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
        (norm): InstanceNorm3d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
    )
    (CB3d): CB3d(
      (conv1): CNA3d(
        (conv): Conv3d(64, 80, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        (norm): InstanceNorm3d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (conv2): CNA3d(
        (conv): Conv3d(80, 80, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        (norm): InstanceNorm3d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
    )
  )
  (bottleneck4): Down(
    (CB2d): CB3d(
      (conv1): CNA3d(
        (conv): Conv3d(80, 96, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
        (norm): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (conv2): CNA3d(
        (conv): Conv3d(96, 96, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
        (norm): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
    )
    (CB3d): CB3d(
      (conv1): CNA3d(
        (conv): Conv3d(80, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        (norm): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (conv2): CNA3d(
        (conv): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        (norm): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
    )
    (fuse): GatedFMU(
      (sg): SpatialGate(
        (conv): Conv3d(2, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      )
    )
  )
  (up41): Up(
    (CB3d): CB3d(
      (conv1): CNA3d(
        (conv): Conv3d(176, 80, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        (norm): InstanceNorm3d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (conv2): CNA3d(
        (conv): Conv3d(80, 80, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        (norm): InstanceNorm3d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
    )
    (fuse): GatedFMU(
      (sg): SpatialGate(
        (conv): Conv3d(2, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      )
    )
    (fuse_skip): GatedFMU(
      (sg): SpatialGate(
        (conv): Conv3d(2, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      )
    )
    (fuse_up): GatedFMU(
      (sg): SpatialGate(
        (conv): Conv3d(2, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      )
    )
  )
  (bottleneck5): Down(
    (CB3d): CB3d(
      (conv1): CNA3d(
        (conv): Conv3d(80, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        (norm): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (conv2): CNA3d(
        (conv): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        (norm): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (activation): LeakyReLU(negative_slope=0.01, inplace=True)
      )
    )
  )
  (outputs): ModuleList(
    (0): Conv3d(32, 3, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1-2): 2 x Conv3d(48, 3, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3-4): 2 x Conv3d(64, 3, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (5-6): 2 x Conv3d(80, 3, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
) 
2025-11-18 22:16:05.480158: 
 
2025-11-18 22:16:05.487925: 
epoch:  0 
2025-11-18 22:18:08.655988: train loss : 0.0673 
2025-11-18 22:18:16.531501: validation loss: 0.0223 
2025-11-18 22:18:16.534664: Average global foreground Dice: [0.6786, 0.0] 
2025-11-18 22:18:16.536554: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2025-11-18 22:18:17.005172: lr: 0.00994 
2025-11-18 22:18:17.008343: [W&B] Logged epoch 0 to WandB 
2025-11-18 22:18:17.009690: [W&B] Epoch 0, continue_training=True, max_epochs=150 
2025-11-18 22:18:17.011122: This epoch took 131.458781 s
 
2025-11-18 22:18:17.012667: 
epoch:  1 
2025-11-18 22:20:02.738472: train loss : -0.1585 
2025-11-18 22:20:11.764292: validation loss: -0.0896 
2025-11-18 22:20:11.769995: Average global foreground Dice: [0.7836, 0.0] 
2025-11-18 22:20:11.773165: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2025-11-18 22:20:12.521818: lr: 0.00988 
2025-11-18 22:20:12.582667: saving checkpoint... 
2025-11-18 22:20:12.799013: done, saving took 0.27 seconds 
2025-11-18 22:20:12.819279: [W&B] Logged epoch 1 to WandB 
2025-11-18 22:20:12.821170: [W&B] Epoch 1, continue_training=True, max_epochs=150 
2025-11-18 22:20:12.822815: This epoch took 115.808542 s
 
2025-11-18 22:20:12.824168: 
epoch:  2 
2025-11-18 22:21:58.005320: train loss : -0.1865 
2025-11-18 22:22:06.099471: validation loss: -0.2458 
2025-11-18 22:22:06.102586: Average global foreground Dice: [0.8775, 0.1483] 
2025-11-18 22:22:06.104617: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2025-11-18 22:22:06.678862: lr: 0.00982 
2025-11-18 22:22:06.721097: saving checkpoint... 
2025-11-18 22:22:06.956922: done, saving took 0.28 seconds 
2025-11-18 22:22:06.963863: [W&B] Logged epoch 2 to WandB 
2025-11-18 22:22:06.965322: [W&B] Epoch 2, continue_training=True, max_epochs=150 
2025-11-18 22:22:06.966374: This epoch took 114.136922 s
 
2025-11-18 22:22:06.967721: 
epoch:  3 
2025-11-18 22:23:53.954495: train loss : -0.2127 
2025-11-18 22:24:03.323262: validation loss: -0.2639 
2025-11-18 22:24:03.327343: Average global foreground Dice: [0.8702, 0.2623] 
2025-11-18 22:24:03.332417: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2025-11-18 22:24:03.904517: lr: 0.00976 
2025-11-18 22:24:03.948066: saving checkpoint... 
2025-11-18 22:24:04.095732: done, saving took 0.19 seconds 
2025-11-18 22:24:04.100648: [W&B] Logged epoch 3 to WandB 
2025-11-18 22:24:04.102137: [W&B] Epoch 3, continue_training=True, max_epochs=150 
2025-11-18 22:24:04.103447: This epoch took 117.134095 s
 
2025-11-18 22:24:04.104810: 
epoch:  4 
2025-11-18 22:25:48.260451: train loss : -0.2701 
2025-11-18 22:25:56.577232: validation loss: -0.3439 
2025-11-18 22:25:56.580671: Average global foreground Dice: [0.8886, 0.3478] 
2025-11-18 22:25:56.583103: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2025-11-18 22:25:57.227374: lr: 0.009699 
2025-11-18 22:25:57.287536: saving checkpoint... 
2025-11-18 22:25:57.543107: done, saving took 0.31 seconds 
2025-11-18 22:25:57.550561: [W&B] Logged epoch 4 to WandB 
2025-11-18 22:25:57.552410: [W&B] Epoch 4, continue_training=True, max_epochs=150 
2025-11-18 22:25:57.554339: This epoch took 113.447589 s
 
2025-11-18 22:25:57.556642: 
epoch:  5 
2025-11-18 22:27:46.566098: train loss : -0.3270 
2025-11-18 22:27:53.541610: validation loss: -0.3334 
2025-11-18 22:27:53.545277: Average global foreground Dice: [0.8653, 0.4078] 
2025-11-18 22:27:53.547383: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2025-11-18 22:27:54.072891: lr: 0.009639 
2025-11-18 22:27:54.111975: saving checkpoint... 
2025-11-18 22:27:54.377936: done, saving took 0.30 seconds 
2025-11-18 22:27:54.383959: [W&B] Logged epoch 5 to WandB 
2025-11-18 22:27:54.385283: [W&B] Epoch 5, continue_training=True, max_epochs=150 
2025-11-18 22:27:54.386601: This epoch took 116.827210 s
 
2025-11-18 22:27:54.387872: 
epoch:  6 
2025-11-18 22:29:42.117623: train loss : -0.3056 
2025-11-18 22:29:50.280900: validation loss: -0.2706 
2025-11-18 22:29:50.284486: Average global foreground Dice: [0.8358, 0.3795] 
2025-11-18 22:29:50.286536: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2025-11-18 22:29:50.833522: lr: 0.009579 
2025-11-18 22:29:50.873519: saving checkpoint... 
2025-11-18 22:29:51.127859: done, saving took 0.29 seconds 
2025-11-18 22:29:51.133654: [W&B] Logged epoch 6 to WandB 
2025-11-18 22:29:51.135092: [W&B] Epoch 6, continue_training=True, max_epochs=150 
2025-11-18 22:29:51.136372: This epoch took 116.746979 s
 
2025-11-18 22:29:51.137664: 
epoch:  7 
2025-11-18 22:31:48.158320: train loss : -0.3249 
2025-11-18 22:31:56.774015: validation loss: -0.3575 
2025-11-18 22:31:56.777903: Average global foreground Dice: [0.8765, 0.4502] 
2025-11-18 22:31:56.780165: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2025-11-18 22:31:57.487653: lr: 0.009519 
2025-11-18 22:31:57.534841: saving checkpoint... 
2025-11-18 22:31:57.758860: done, saving took 0.27 seconds 
2025-11-18 22:31:57.763858: [W&B] Logged epoch 7 to WandB 
2025-11-18 22:31:57.764966: [W&B] Epoch 7, continue_training=True, max_epochs=150 
2025-11-18 22:31:57.766286: This epoch took 126.626574 s
 
2025-11-18 22:31:57.767338: 
epoch:  8 
2025-11-18 22:33:43.835701: train loss : -0.3811 
2025-11-18 22:33:50.740116: validation loss: -0.3558 
2025-11-18 22:33:50.743462: Average global foreground Dice: [0.8743, 0.4569] 
2025-11-18 22:33:50.745443: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2025-11-18 22:33:51.293099: lr: 0.009458 
2025-11-18 22:33:51.319151: saving checkpoint... 
2025-11-18 22:33:51.507972: done, saving took 0.21 seconds 
2025-11-18 22:33:51.512990: [W&B] Logged epoch 8 to WandB 
2025-11-18 22:33:51.514266: [W&B] Epoch 8, continue_training=True, max_epochs=150 
2025-11-18 22:33:51.515455: This epoch took 113.746900 s
 
2025-11-18 22:33:51.516610: 
epoch:  9 
2025-11-18 22:35:37.327577: train loss : -0.4340 
2025-11-18 22:35:46.016268: validation loss: -0.3926 
2025-11-18 22:35:46.020060: Average global foreground Dice: [0.866, 0.4178] 
2025-11-18 22:35:46.022352: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2025-11-18 22:35:46.589750: lr: 0.009398 
2025-11-18 22:35:46.613795: saving checkpoint... 
2025-11-18 22:35:46.872878: done, saving took 0.28 seconds 
2025-11-18 22:35:46.879798: [W&B] Logged epoch 9 to WandB 
2025-11-18 22:35:46.881164: [W&B] Epoch 9, continue_training=True, max_epochs=150 
2025-11-18 22:35:46.882252: This epoch took 115.363738 s
 
2025-11-18 22:35:46.883382: 
epoch:  10 
2025-11-18 22:37:30.683651: train loss : -0.4048 
2025-11-18 22:37:39.415168: validation loss: -0.3977 
2025-11-18 22:37:39.418065: Average global foreground Dice: [0.8748, 0.5304] 
2025-11-18 22:37:39.419960: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2025-11-18 22:37:40.011791: lr: 0.009338 
2025-11-18 22:37:40.033879: saving checkpoint... 
2025-11-18 22:37:40.236806: done, saving took 0.22 seconds 
2025-11-18 22:37:40.242009: [W&B] Logged epoch 10 to WandB 
2025-11-18 22:37:40.243201: [W&B] Epoch 10, continue_training=True, max_epochs=150 
2025-11-18 22:37:40.244462: This epoch took 113.359323 s
 
2025-11-18 22:37:40.245659: 
epoch:  11 
2025-11-18 22:39:29.498814: train loss : -0.4002 
2025-11-18 22:39:38.965736: validation loss: -0.4142 
2025-11-18 22:39:38.969363: Average global foreground Dice: [0.9002, 0.4141] 
2025-11-18 22:39:38.971735: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2025-11-18 22:39:39.513171: lr: 0.009277 
2025-11-18 22:39:39.539612: saving checkpoint... 
2025-11-18 22:39:39.803638: done, saving took 0.29 seconds 
2025-11-18 22:39:39.810596: [W&B] Logged epoch 11 to WandB 
2025-11-18 22:39:39.812191: [W&B] Epoch 11, continue_training=True, max_epochs=150 
2025-11-18 22:39:39.813614: This epoch took 119.566226 s
 
2025-11-18 22:39:39.814950: 
epoch:  12 
2025-11-18 22:41:27.773740: train loss : -0.3863 
2025-11-18 22:41:35.959506: validation loss: -0.3199 
2025-11-18 22:41:35.965686: Average global foreground Dice: [0.8618, 0.2707] 
2025-11-18 22:41:35.968525: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2025-11-18 22:41:36.569361: lr: 0.009217 
2025-11-18 22:41:36.594193: saving checkpoint... 
2025-11-18 22:41:36.885511: done, saving took 0.31 seconds 
2025-11-18 22:41:36.894892: [W&B] Logged epoch 12 to WandB 
2025-11-18 22:41:36.897727: [W&B] Epoch 12, continue_training=True, max_epochs=150 
2025-11-18 22:41:36.900775: This epoch took 117.083854 s
 
2025-11-18 22:41:36.902311: 
epoch:  13 
2025-11-18 22:43:26.558266: train loss : -0.4268 
2025-11-18 22:43:36.037037: validation loss: -0.4095 
2025-11-18 22:43:36.041483: Average global foreground Dice: [0.869, 0.481] 
2025-11-18 22:43:36.043574: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2025-11-18 22:43:36.640832: lr: 0.009156 
2025-11-18 22:43:36.664328: saving checkpoint... 
2025-11-18 22:43:36.894445: done, saving took 0.25 seconds 
2025-11-18 22:43:36.900026: [W&B] Logged epoch 13 to WandB 
2025-11-18 22:43:36.901583: [W&B] Epoch 13, continue_training=True, max_epochs=150 
2025-11-18 22:43:36.902736: This epoch took 119.998337 s
 
2025-11-18 22:43:36.903935: 
epoch:  14 
2025-11-18 22:45:21.699941: train loss : -0.4390 
2025-11-18 22:45:30.128858: validation loss: -0.3432 
2025-11-18 22:45:30.132102: Average global foreground Dice: [0.8844, 0.374] 
2025-11-18 22:45:30.134065: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2025-11-18 22:45:30.688385: lr: 0.009095 
2025-11-18 22:45:30.714794: saving checkpoint... 
2025-11-18 22:45:30.972362: done, saving took 0.28 seconds 
2025-11-18 22:45:30.978183: [W&B] Logged epoch 14 to WandB 
2025-11-18 22:45:30.979685: [W&B] Epoch 14, continue_training=True, max_epochs=150 
2025-11-18 22:45:30.980974: This epoch took 114.074892 s
 
2025-11-18 22:45:30.982121: 
epoch:  15 
2025-11-18 22:47:17.272288: train loss : -0.4309 
2025-11-18 22:47:25.559532: validation loss: -0.4174 
2025-11-18 22:47:25.563513: Average global foreground Dice: [0.8798, 0.5312] 
2025-11-18 22:47:25.566056: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2025-11-18 22:47:26.125076: lr: 0.009035 
2025-11-18 22:47:26.150769: saving checkpoint... 
2025-11-18 22:47:26.381150: done, saving took 0.25 seconds 
2025-11-18 22:47:26.418813: [W&B] Logged epoch 15 to WandB 
2025-11-18 22:47:26.420729: [W&B] Epoch 15, continue_training=True, max_epochs=150 
2025-11-18 22:47:26.422335: This epoch took 115.438602 s
 
2025-11-18 22:47:26.423457: 
epoch:  16 
2025-11-18 22:49:11.195659: train loss : -0.4480 
2025-11-18 22:49:20.361195: validation loss: -0.4584 
2025-11-18 22:49:20.365607: Average global foreground Dice: [0.9175, 0.4054] 
2025-11-18 22:49:20.369009: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2025-11-18 22:49:20.973642: lr: 0.008974 
2025-11-18 22:49:21.012294: saving checkpoint... 
2025-11-18 22:49:21.275839: done, saving took 0.30 seconds 
2025-11-18 22:49:21.282324: [W&B] Logged epoch 16 to WandB 
2025-11-18 22:49:21.283680: [W&B] Epoch 16, continue_training=True, max_epochs=150 
2025-11-18 22:49:21.285151: This epoch took 114.860108 s
 
2025-11-18 22:49:21.286324: 
epoch:  17 
2025-11-18 22:51:06.208177: train loss : -0.4669 
2025-11-18 22:51:13.563129: validation loss: -0.4446 
2025-11-18 22:51:13.566636: Average global foreground Dice: [0.9079, 0.5108] 
2025-11-18 22:51:13.568575: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2025-11-18 22:51:14.137682: lr: 0.008913 
2025-11-18 22:51:14.161156: saving checkpoint... 
2025-11-18 22:51:14.687966: done, saving took 0.55 seconds 
2025-11-18 22:51:14.693541: [W&B] Logged epoch 17 to WandB 
2025-11-18 22:51:14.694830: [W&B] Epoch 17, continue_training=True, max_epochs=150 
2025-11-18 22:51:14.695996: This epoch took 113.407965 s
 
2025-11-18 22:51:14.697090: 
epoch:  18 
2025-11-18 22:53:05.870133: train loss : -0.4791 
2025-11-18 22:53:14.734750: validation loss: -0.3869 
2025-11-18 22:53:14.737711: Average global foreground Dice: [0.8858, 0.4632] 
2025-11-18 22:53:14.739613: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2025-11-18 22:53:15.318492: lr: 0.008852 
2025-11-18 22:53:15.345319: saving checkpoint... 
2025-11-18 22:53:15.581865: done, saving took 0.26 seconds 
2025-11-18 22:53:15.587501: [W&B] Logged epoch 18 to WandB 
2025-11-18 22:53:15.588863: [W&B] Epoch 18, continue_training=True, max_epochs=150 
2025-11-18 22:53:15.590090: This epoch took 120.891530 s
 
2025-11-18 22:53:15.591115: 
epoch:  19 
2025-11-18 22:55:09.004972: train loss : -0.4804 
2025-11-18 22:55:18.340641: validation loss: -0.5085 
2025-11-18 22:55:18.353718: Average global foreground Dice: [0.923, 0.5197] 
2025-11-18 22:55:18.361504: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2025-11-18 22:55:19.105469: lr: 0.008792 
2025-11-18 22:55:19.129087: saving checkpoint... 
2025-11-18 22:55:19.317705: done, saving took 0.21 seconds 
2025-11-18 22:55:19.324193: [W&B] Logged epoch 19 to WandB 
2025-11-18 22:55:19.325693: [W&B] Epoch 19, continue_training=True, max_epochs=150 
2025-11-18 22:55:19.326917: This epoch took 123.734390 s
 
2025-11-18 22:55:19.328208: 
epoch:  20 
2025-11-18 22:57:03.789984: train loss : -0.4448 
2025-11-18 22:57:11.680345: validation loss: -0.3833 
2025-11-18 22:57:11.683204: Average global foreground Dice: [0.8751, 0.4365] 
2025-11-18 22:57:11.684875: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2025-11-18 22:57:12.294163: lr: 0.008731 
2025-11-18 22:57:12.320289: saving checkpoint... 
2025-11-18 22:57:12.606017: done, saving took 0.31 seconds 
2025-11-18 22:57:12.612572: [W&B] Logged epoch 20 to WandB 
2025-11-18 22:57:12.614243: [W&B] Epoch 20, continue_training=True, max_epochs=150 
2025-11-18 22:57:12.615695: This epoch took 113.285610 s
 
2025-11-18 22:57:12.617301: 
epoch:  21 
2025-11-18 22:58:58.642477: train loss : -0.4600 
2025-11-18 22:59:06.394787: validation loss: -0.4642 
2025-11-18 22:59:06.398431: Average global foreground Dice: [0.8931, 0.4975] 
2025-11-18 22:59:06.401128: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2025-11-18 22:59:06.980688: lr: 0.00867 
2025-11-18 22:59:07.003345: saving checkpoint... 
2025-11-18 22:59:07.255987: done, saving took 0.27 seconds 
2025-11-18 22:59:07.261334: [W&B] Logged epoch 21 to WandB 
2025-11-18 22:59:07.262736: [W&B] Epoch 21, continue_training=True, max_epochs=150 
2025-11-18 22:59:07.263880: This epoch took 114.644328 s
 
2025-11-18 22:59:07.265120: 
epoch:  22 
2025-11-18 23:00:51.460126: train loss : -0.4789 
2025-11-18 23:01:00.027347: validation loss: -0.4751 
2025-11-18 23:01:00.031260: Average global foreground Dice: [0.9051, 0.413] 
2025-11-18 23:01:00.033992: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2025-11-18 23:01:00.623335: lr: 0.008609 
2025-11-18 23:01:00.649624: saving checkpoint... 
2025-11-18 23:01:00.913931: done, saving took 0.29 seconds 
2025-11-18 23:01:00.920640: [W&B] Logged epoch 22 to WandB 
2025-11-18 23:01:00.922421: [W&B] Epoch 22, continue_training=True, max_epochs=150 
2025-11-18 23:01:00.923819: This epoch took 113.656895 s
 
2025-11-18 23:01:00.925160: 
epoch:  23 
2025-11-18 23:02:47.649873: train loss : -0.4842 
2025-11-18 23:02:57.337960: validation loss: -0.4515 
2025-11-18 23:02:57.343309: Average global foreground Dice: [0.9033, 0.4116] 
2025-11-18 23:02:57.347436: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2025-11-18 23:02:58.101883: lr: 0.008548 
2025-11-18 23:02:58.148985: saving checkpoint... 
2025-11-18 23:02:58.406923: done, saving took 0.29 seconds 
2025-11-18 23:02:58.416670: [W&B] Logged epoch 23 to WandB 
2025-11-18 23:02:58.418409: [W&B] Epoch 23, continue_training=True, max_epochs=150 
2025-11-18 23:02:58.419856: This epoch took 117.492549 s
 
2025-11-18 23:02:58.421435: 
epoch:  24 
2025-11-18 23:04:51.916311: train loss : -0.5002 
2025-11-18 23:05:00.904460: validation loss: -0.4633 
2025-11-18 23:05:00.909064: Average global foreground Dice: [0.9085, 0.3506] 
2025-11-18 23:05:00.911183: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2025-11-18 23:05:01.805200: lr: 0.008487 
2025-11-18 23:05:01.808391: [W&B] Logged epoch 24 to WandB 
2025-11-18 23:05:01.810005: [W&B] Epoch 24, continue_training=True, max_epochs=150 
2025-11-18 23:05:01.811928: This epoch took 123.388403 s
 
2025-11-18 23:05:01.813590: 
epoch:  25 
2025-11-18 23:06:55.406709: train loss : -0.4912 
2025-11-18 23:07:04.658597: validation loss: -0.4427 
2025-11-18 23:07:04.669047: Average global foreground Dice: [0.8971, 0.3818] 
2025-11-18 23:07:04.677682: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2025-11-18 23:07:05.349570: lr: 0.008426 
2025-11-18 23:07:05.401401: saving checkpoint... 
2025-11-18 23:07:05.575727: done, saving took 0.22 seconds 
2025-11-18 23:07:05.582704: [W&B] Logged epoch 25 to WandB 
2025-11-18 23:07:05.584836: [W&B] Epoch 25, continue_training=True, max_epochs=150 
2025-11-18 23:07:05.586280: This epoch took 123.770753 s
 
2025-11-18 23:07:05.588042: 
epoch:  26 
2025-11-18 23:08:54.225490: train loss : -0.5363 
2025-11-18 23:09:03.914072: validation loss: -0.4789 
2025-11-18 23:09:03.921573: Average global foreground Dice: [0.9138, 0.4421] 
2025-11-18 23:09:03.929370: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2025-11-18 23:09:04.656027: lr: 0.008364 
2025-11-18 23:09:04.682379: saving checkpoint... 
2025-11-18 23:09:04.852901: done, saving took 0.19 seconds 
2025-11-18 23:09:04.860097: [W&B] Logged epoch 26 to WandB 
2025-11-18 23:09:04.861707: [W&B] Epoch 26, continue_training=True, max_epochs=150 
2025-11-18 23:09:04.863100: This epoch took 119.272726 s
 
2025-11-18 23:09:04.864420: 
epoch:  27 
2025-11-18 23:10:54.745372: train loss : -0.5172 
2025-11-18 23:11:03.581140: validation loss: -0.5344 
2025-11-18 23:11:03.584391: Average global foreground Dice: [0.9185, 0.5082] 
2025-11-18 23:11:03.587149: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2025-11-18 23:11:04.218060: lr: 0.008303 
2025-11-18 23:11:04.240031: saving checkpoint... 
2025-11-18 23:11:04.483012: done, saving took 0.26 seconds 
2025-11-18 23:11:04.488398: [W&B] Logged epoch 27 to WandB 
2025-11-18 23:11:04.489745: [W&B] Epoch 27, continue_training=True, max_epochs=150 
2025-11-18 23:11:04.490926: This epoch took 119.624490 s
 
2025-11-18 23:11:04.492137: 
epoch:  28 
2025-11-18 23:12:55.229373: train loss : -0.4845 
2025-11-18 23:13:05.123376: validation loss: -0.4466 
2025-11-18 23:13:05.129428: Average global foreground Dice: [0.8981, 0.3902] 
2025-11-18 23:13:05.134134: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2025-11-18 23:13:05.798797: lr: 0.008242 
2025-11-18 23:13:05.802058: [W&B] Logged epoch 28 to WandB 
2025-11-18 23:13:05.803555: [W&B] Epoch 28, continue_training=True, max_epochs=150 
2025-11-18 23:13:05.805145: This epoch took 121.311258 s
 
2025-11-18 23:13:05.806905: 
epoch:  29 
2025-11-18 23:14:59.582829: train loss : -0.5092 
2025-11-18 23:15:08.299321: validation loss: -0.5507 
2025-11-18 23:15:08.316306: Average global foreground Dice: [0.9298, 0.6293] 
2025-11-18 23:15:08.320909: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2025-11-18 23:15:08.970047: lr: 0.008181 
2025-11-18 23:15:09.003179: saving checkpoint... 
2025-11-18 23:15:09.245590: done, saving took 0.27 seconds 
2025-11-18 23:15:09.251404: [W&B] Logged epoch 29 to WandB 
2025-11-18 23:15:09.252832: [W&B] Epoch 29, continue_training=True, max_epochs=150 
2025-11-18 23:15:09.254435: This epoch took 123.445965 s
 
2025-11-18 23:15:09.255828: 
epoch:  30 
2025-11-18 23:16:54.313602: train loss : -0.5066 
2025-11-18 23:17:01.789481: validation loss: -0.5391 
2025-11-18 23:17:01.792966: Average global foreground Dice: [0.9179, 0.5108] 
2025-11-18 23:17:01.795060: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2025-11-18 23:17:02.456193: lr: 0.008119 
2025-11-18 23:17:02.480366: saving checkpoint... 
2025-11-18 23:17:02.702457: done, saving took 0.24 seconds 
2025-11-18 23:17:02.707498: [W&B] Logged epoch 30 to WandB 
2025-11-18 23:17:02.708848: [W&B] Epoch 30, continue_training=True, max_epochs=150 
2025-11-18 23:17:02.710305: This epoch took 113.451391 s
 
2025-11-18 23:17:02.711575: 
epoch:  31 
2025-11-18 23:18:48.763174: train loss : -0.5316 
2025-11-18 23:18:58.738434: validation loss: -0.5000 
2025-11-18 23:18:58.744672: Average global foreground Dice: [0.9202, 0.4672] 
2025-11-18 23:18:58.746705: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2025-11-18 23:18:59.366560: lr: 0.008058 
2025-11-18 23:18:59.393531: saving checkpoint... 
2025-11-18 23:18:59.639946: done, saving took 0.27 seconds 
2025-11-18 23:18:59.647469: [W&B] Logged epoch 31 to WandB 
2025-11-18 23:18:59.649025: [W&B] Epoch 31, continue_training=True, max_epochs=150 
2025-11-18 23:18:59.650305: This epoch took 116.936903 s
 
2025-11-18 23:18:59.651715: 
epoch:  32 
2025-11-18 23:20:50.363259: train loss : -0.5156 
2025-11-18 23:20:59.984392: validation loss: -0.5061 
2025-11-18 23:20:59.988898: Average global foreground Dice: [0.9096, 0.5886] 
2025-11-18 23:20:59.992336: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2025-11-18 23:21:00.804660: lr: 0.007996 
2025-11-18 23:21:00.831847: saving checkpoint... 
2025-11-18 23:21:01.063872: done, saving took 0.26 seconds 
2025-11-18 23:21:01.072716: [W&B] Logged epoch 32 to WandB 
2025-11-18 23:21:01.074967: [W&B] Epoch 32, continue_training=True, max_epochs=150 
2025-11-18 23:21:01.076381: This epoch took 121.422571 s
 
2025-11-18 23:21:01.077773: 
epoch:  33 
2025-11-18 23:22:48.014583: train loss : -0.5261 
2025-11-18 23:22:56.060723: validation loss: -0.4558 
2025-11-18 23:22:56.065006: Average global foreground Dice: [0.8784, 0.477] 
2025-11-18 23:22:56.067585: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2025-11-18 23:22:56.719452: lr: 0.007935 
2025-11-18 23:22:56.745501: saving checkpoint... 
2025-11-18 23:22:56.913596: done, saving took 0.19 seconds 
2025-11-18 23:22:56.919437: [W&B] Logged epoch 33 to WandB 
2025-11-18 23:22:56.920949: [W&B] Epoch 33, continue_training=True, max_epochs=150 
2025-11-18 23:22:56.922426: This epoch took 115.842656 s
 
2025-11-18 23:22:56.923682: 
epoch:  34 
2025-11-18 23:24:47.287484: train loss : -0.5395 
2025-11-18 23:24:56.734523: validation loss: -0.4922 
2025-11-18 23:24:56.744747: Average global foreground Dice: [0.9256, 0.4583] 
2025-11-18 23:24:56.749814: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2025-11-18 23:24:57.540036: lr: 0.007873 
2025-11-18 23:24:57.573631: saving checkpoint... 
2025-11-18 23:24:57.840395: done, saving took 0.30 seconds 
2025-11-18 23:24:57.853613: [W&B] Logged epoch 34 to WandB 
2025-11-18 23:24:57.856297: [W&B] Epoch 34, continue_training=True, max_epochs=150 
2025-11-18 23:24:57.857985: This epoch took 120.932153 s
 
2025-11-18 23:24:57.859437: 
epoch:  35 
2025-11-18 23:26:49.658512: train loss : -0.5197 
2025-11-18 23:26:59.013834: validation loss: -0.5363 
2025-11-18 23:26:59.017964: Average global foreground Dice: [0.9225, 0.5862] 
2025-11-18 23:26:59.020663: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2025-11-18 23:26:59.665426: lr: 0.007811 
2025-11-18 23:26:59.689402: saving checkpoint... 
2025-11-18 23:26:59.940503: done, saving took 0.27 seconds 
2025-11-18 23:26:59.948295: [W&B] Logged epoch 35 to WandB 
2025-11-18 23:26:59.949867: [W&B] Epoch 35, continue_training=True, max_epochs=150 
2025-11-18 23:26:59.951022: This epoch took 122.089109 s
 
2025-11-18 23:26:59.952155: 
epoch:  36 
2025-11-18 23:28:49.402880: train loss : -0.5427 
2025-11-18 23:28:57.847601: validation loss: -0.4412 
2025-11-18 23:28:57.854205: Average global foreground Dice: [0.9036, 0.4396] 
2025-11-18 23:28:57.858050: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2025-11-18 23:28:58.591731: lr: 0.00775 
2025-11-18 23:28:58.594745: [W&B] Logged epoch 36 to WandB 
2025-11-18 23:28:58.596115: [W&B] Epoch 36, continue_training=True, max_epochs=150 
2025-11-18 23:28:58.597312: This epoch took 118.642975 s
 
2025-11-18 23:28:58.598550: 
epoch:  37 
2025-11-18 23:30:50.481156: train loss : -0.5255 
2025-11-18 23:30:58.481805: validation loss: -0.5955 
2025-11-18 23:30:58.484950: Average global foreground Dice: [0.9354, 0.6722] 
2025-11-18 23:30:58.486964: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2025-11-18 23:30:59.054594: lr: 0.007688 
2025-11-18 23:30:59.078207: saving checkpoint... 
2025-11-18 23:30:59.294631: done, saving took 0.24 seconds 
2025-11-18 23:30:59.299553: [W&B] Logged epoch 37 to WandB 
2025-11-18 23:30:59.300621: [W&B] Epoch 37, continue_training=True, max_epochs=150 
2025-11-18 23:30:59.301667: This epoch took 120.701453 s
 
2025-11-18 23:30:59.302599: 
epoch:  38 
2025-11-18 23:32:52.921096: train loss : -0.5143 
2025-11-18 23:33:01.689783: validation loss: -0.5219 
2025-11-18 23:33:01.692914: Average global foreground Dice: [0.925, 0.5021] 
2025-11-18 23:33:01.695029: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2025-11-18 23:33:02.325627: lr: 0.007626 
2025-11-18 23:33:02.359051: saving checkpoint... 
2025-11-18 23:33:02.599777: done, saving took 0.27 seconds 
2025-11-18 23:33:02.606908: [W&B] Logged epoch 38 to WandB 
2025-11-18 23:33:02.608555: [W&B] Epoch 38, continue_training=True, max_epochs=150 
2025-11-18 23:33:02.610031: This epoch took 123.306068 s
 
2025-11-18 23:33:02.611852: 
epoch:  39 
2025-11-18 23:34:52.816187: train loss : -0.5134 
2025-11-18 23:35:02.693527: validation loss: -0.5226 
2025-11-18 23:35:02.703155: Average global foreground Dice: [0.9193, 0.6327] 
2025-11-18 23:35:02.708355: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2025-11-18 23:35:03.534030: lr: 0.007564 
2025-11-18 23:35:03.562990: saving checkpoint... 
2025-11-18 23:35:03.775440: done, saving took 0.24 seconds 
2025-11-18 23:35:03.781901: [W&B] Logged epoch 39 to WandB 
2025-11-18 23:35:03.783274: [W&B] Epoch 39, continue_training=True, max_epochs=150 
2025-11-18 23:35:03.784433: This epoch took 121.170277 s
 
2025-11-18 23:35:03.785712: 
epoch:  40 
2025-11-18 23:36:55.127757: train loss : -0.5558 
2025-11-18 23:37:03.793231: validation loss: -0.6039 
2025-11-18 23:37:03.796465: Average global foreground Dice: [0.9368, 0.7241] 
2025-11-18 23:37:03.798642: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2025-11-18 23:37:04.448755: lr: 0.007502 
2025-11-18 23:37:04.475377: saving checkpoint... 
2025-11-18 23:37:04.730935: done, saving took 0.28 seconds 
2025-11-18 23:37:04.737334: [W&B] Logged epoch 40 to WandB 
2025-11-18 23:37:04.739239: [W&B] Epoch 40, continue_training=True, max_epochs=150 
2025-11-18 23:37:04.740818: This epoch took 120.953276 s
 
2025-11-18 23:37:04.742359: 
epoch:  41 
2025-11-18 23:38:54.363855: train loss : -0.5573 
2025-11-18 23:39:03.806212: validation loss: -0.4858 
2025-11-18 23:39:03.809942: Average global foreground Dice: [0.9187, 0.388] 
2025-11-18 23:39:03.812160: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2025-11-18 23:39:04.456947: lr: 0.00744 
2025-11-18 23:39:04.460146: [W&B] Logged epoch 41 to WandB 
2025-11-18 23:39:04.461705: [W&B] Epoch 41, continue_training=True, max_epochs=150 
2025-11-18 23:39:04.463310: This epoch took 119.719042 s
 
2025-11-18 23:39:04.464792: 
epoch:  42 
2025-11-18 23:41:00.756842: train loss : -0.5493 
2025-11-18 23:41:10.470590: validation loss: -0.5218 
2025-11-18 23:41:10.474135: Average global foreground Dice: [0.9248, 0.4395] 
2025-11-18 23:41:10.476124: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2025-11-18 23:41:11.057973: lr: 0.007378 
2025-11-18 23:41:11.061412: [W&B] Logged epoch 42 to WandB 
2025-11-18 23:41:11.062663: [W&B] Epoch 42, continue_training=True, max_epochs=150 
2025-11-18 23:41:11.064001: This epoch took 126.597162 s
 
2025-11-18 23:41:11.065400: 
epoch:  43 
2025-11-18 23:43:04.465450: train loss : -0.5570 
2025-11-18 23:43:13.883447: validation loss: -0.5460 
2025-11-18 23:43:13.886472: Average global foreground Dice: [0.9332, 0.535] 
2025-11-18 23:43:13.888853: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2025-11-18 23:43:14.541623: lr: 0.007316 
2025-11-18 23:43:14.544641: [W&B] Logged epoch 43 to WandB 
2025-11-18 23:43:14.545729: [W&B] Epoch 43, continue_training=True, max_epochs=150 
2025-11-18 23:43:14.546904: This epoch took 123.479769 s
 
2025-11-18 23:43:14.548460: 
epoch:  44 
2025-11-18 23:45:03.586636: train loss : -0.5659 
2025-11-18 23:45:12.268578: validation loss: -0.4919 
2025-11-18 23:45:12.274273: Average global foreground Dice: [0.9308, 0.3014] 
2025-11-18 23:45:12.279036: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2025-11-18 23:45:13.268586: lr: 0.007254 
2025-11-18 23:45:13.277904: [W&B] Logged epoch 44 to WandB 
2025-11-18 23:45:13.281492: [W&B] Epoch 44, continue_training=True, max_epochs=150 
2025-11-18 23:45:13.286137: This epoch took 118.735723 s
 
2025-11-18 23:45:13.290458: 
epoch:  45 
2025-11-18 23:47:01.646789: train loss : -0.5615 
2025-11-18 23:47:09.117311: validation loss: -0.5634 
2025-11-18 23:47:09.119852: Average global foreground Dice: [0.937, 0.4506] 
2025-11-18 23:47:09.121774: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2025-11-18 23:47:09.708076: lr: 0.007192 
2025-11-18 23:47:09.710758: [W&B] Logged epoch 45 to WandB 
2025-11-18 23:47:09.711874: [W&B] Epoch 45, continue_training=True, max_epochs=150 
2025-11-18 23:47:09.712962: This epoch took 116.420507 s
 
2025-11-18 23:47:09.714041: 
epoch:  46 
2025-11-18 23:48:58.773100: train loss : -0.5666 
2025-11-18 23:49:08.260957: validation loss: -0.5640 
2025-11-18 23:49:08.270950: Average global foreground Dice: [0.9416, 0.451] 
2025-11-18 23:49:08.273202: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2025-11-18 23:49:09.026415: lr: 0.00713 
2025-11-18 23:49:09.037492: [W&B] Logged epoch 46 to WandB 
2025-11-18 23:49:09.041707: [W&B] Epoch 46, continue_training=True, max_epochs=150 
2025-11-18 23:49:09.043620: This epoch took 119.328412 s
 
2025-11-18 23:49:09.046127: 
epoch:  47 
2025-11-18 23:50:53.442471: train loss : -0.5556 
2025-11-18 23:51:02.127289: validation loss: -0.5559 
2025-11-18 23:51:02.130349: Average global foreground Dice: [0.9181, 0.5931] 
2025-11-18 23:51:02.133155: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2025-11-18 23:51:02.811934: lr: 0.007067 
2025-11-18 23:51:02.816650: [W&B] Logged epoch 47 to WandB 
2025-11-18 23:51:02.818721: [W&B] Epoch 47, continue_training=True, max_epochs=150 
2025-11-18 23:51:02.820519: This epoch took 113.771211 s
 
2025-11-18 23:51:02.822681: 
epoch:  48 
2025-11-18 23:52:53.028503: train loss : -0.6081 
2025-11-18 23:53:01.752259: validation loss: -0.5731 
2025-11-18 23:53:01.755947: Average global foreground Dice: [0.9168, 0.6275] 
2025-11-18 23:53:01.758026: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2025-11-18 23:53:02.402722: lr: 0.007005 
2025-11-18 23:53:02.405723: [W&B] Logged epoch 48 to WandB 
2025-11-18 23:53:02.406973: [W&B] Epoch 48, continue_training=True, max_epochs=150 
2025-11-18 23:53:02.408246: This epoch took 119.582062 s
 
2025-11-18 23:53:02.409436: 
epoch:  49 
2025-11-18 23:54:51.737046: train loss : -0.5765 
2025-11-18 23:55:01.494585: validation loss: -0.4398 
2025-11-18 23:55:01.500324: Average global foreground Dice: [0.8952, 0.4633] 
2025-11-18 23:55:01.503013: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2025-11-18 23:55:02.375171: lr: 0.006943 
2025-11-18 23:55:02.379043: saving scheduled checkpoint file... 
2025-11-18 23:55:02.402772: saving checkpoint... 
2025-11-18 23:55:02.604377: done, saving took 0.22 seconds 
2025-11-18 23:55:02.615371: done 
2025-11-18 23:55:02.623334: [W&B] Logged epoch 49 to WandB 
2025-11-18 23:55:02.625870: [W&B] Epoch 49, continue_training=True, max_epochs=150 
2025-11-18 23:55:02.627578: This epoch took 120.215628 s
 
2025-11-18 23:55:02.629809: 
epoch:  50 
